{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933b6eb4",
   "metadata": {},
   "source": [
    "## Read YJMob100k Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d97cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "df = pd.read_csv(\"data/yjmob100k-dataset1.csv.gz\", compression=\"infer\")\n",
    "\n",
    "df = df.sort_values([\"uid\", \"d\", \"t\"]).reset_index(drop=True)\n",
    "\n",
    "df = df.drop_duplicates([\"uid\",\"d\",\"t\",\"x\",\"y\"])\n",
    "\n",
    "mob = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615618b",
   "metadata": {},
   "source": [
    "## Split Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# ========= 1. Required helper functions (copied from previous discussion) =========\n",
    "\n",
    "def ensure_int_types(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Check and convert uid, d, t, x, y to int32 types\"\"\"\n",
    "    need = ('uid','d','t','x','y')\n",
    "    for c in need:\n",
    "        if c not in df.columns:\n",
    "            raise KeyError(f\"Missing required column: {c}\")\n",
    "    for c in need:\n",
    "        if pd.api.types.is_integer_dtype(df[c]) and df[c].isna().sum()==0:\n",
    "            if c in ('x', 'y') and df[c].dtype == np.int64:\n",
    "                continue\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        else:\n",
    "            try:\n",
    "                if df[c].isna().sum() == 0:\n",
    "                    df[c] = df[c].astype(np.int32)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: failed to convert column {c}: {e}\")\n",
    "                \n",
    "def build_place_from_grid(df: pd.DataFrame, G: int = 1, inplace_col: Optional[str] = 'place'):\n",
    "    \"\"\"\n",
    "    Modified version: ensure that 'xg' and 'yg' columns are created and added to df.\n",
    "    \"\"\"\n",
    "    if G <= 0:\n",
    "        raise ValueError(\"G must be a positive integer\")\n",
    "    xg_col, yg_col = 'xg', 'yg'\n",
    "    xg = (df['x'].values // G).astype(np.int64)\n",
    "    yg = (df['y'].values // G).astype(np.int64)\n",
    "    df[xg_col] = xg\n",
    "    df[yg_col] = yg\n",
    "    place = (xg << 32) ^ (yg & 0xffffffff)\n",
    "    if inplace_col:\n",
    "        df[inplace_col] = place\n",
    "    return place.astype(np.int64)\n",
    "\n",
    "def consecutive_run_id_spatial_only(\n",
    "    df_sorted: pd.DataFrame,\n",
    "    by_cols=('uid','d'),\n",
    "    xg_col='xg',\n",
    "    yg_col='yg',\n",
    "    max_grid_gap: int = 3\n",
    "):\n",
    "    \"\"\"\n",
    "    (Core function – improved version: anchor logic)\n",
    "    Use an \"anchor\" logic to compute run_id.\n",
    "    \n",
    "    A new run starts when:\n",
    "    1. by_cols changes (handled automatically by groupby)\n",
    "    2. The Manhattan distance between the current point and the *first point of the run*\n",
    "       (anchor) exceeds max_grid_gap\n",
    "    \n",
    "    Note: this method uses groupby().apply() with an internal Python loop.\n",
    "    It may be slower than .diff() for datasets with a very large number of groups (uid, d).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Define a helper function for .apply()\n",
    "    #    This function processes a single group (e.g., all records for one uid, d)\n",
    "    def _apply_anchor_logic(group: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Apply anchor logic within a single group.\n",
    "        \"\"\"\n",
    "        # Convert columns to NumPy arrays for faster looping\n",
    "        xg_arr = group[xg_col].values\n",
    "        yg_arr = group[yg_col].values\n",
    "        n = len(xg_arr)\n",
    "        \n",
    "        # If group is empty, return empty Series\n",
    "        if n == 0:\n",
    "            return pd.Series([], dtype=np.int32, index=group.index)\n",
    "\n",
    "        # Output run_id array\n",
    "        run_ids = np.empty(n, dtype=np.int32)\n",
    "        \n",
    "        # Initialize state\n",
    "        current_run_id = 0       # current run ID (starting from 0)\n",
    "        anchor_xg = xg_arr[0]    # anchor x of the current run\n",
    "        anchor_yg = yg_arr[0]    # anchor y of the current run\n",
    "        \n",
    "        # 2. Core loop (state machine)\n",
    "        for i in range(n):\n",
    "            current_xg = xg_arr[i]\n",
    "            current_yg = yg_arr[i]\n",
    "            \n",
    "            # Compute Manhattan distance to the anchor\n",
    "            dist_x = abs(current_xg - anchor_xg)\n",
    "            dist_y = abs(current_yg - anchor_yg)\n",
    "            manhattan_dist = dist_x + dist_y\n",
    "            \n",
    "            # 3. Check for run break\n",
    "            if manhattan_dist > max_grid_gap:\n",
    "                current_run_id += 1\n",
    "                # Update anchor to current point\n",
    "                anchor_xg = current_xg\n",
    "                anchor_yg = current_yg\n",
    "            \n",
    "            # Assign run_id\n",
    "            run_ids[i] = current_run_id\n",
    "        \n",
    "        # Wrap NumPy array back into a Series\n",
    "        # Important: use group.index to preserve alignment with original df\n",
    "        return pd.Series(run_ids, index=group.index, dtype=np.int32)\n",
    "\n",
    "    # --- Main logic ---\n",
    "    \n",
    "    # 4. Group by (uid, d) and apply anchor-based run_id logic\n",
    "    #    Assumes df_sorted is already sorted by (uid, d, t)\n",
    "    \n",
    "    run_id_series = df_sorted.groupby(\n",
    "        list(by_cols), \n",
    "        sort=False, \n",
    "        group_keys=False\n",
    "    ).apply(_apply_anchor_logic)\n",
    "    \n",
    "    return run_id_series\n",
    "\n",
    "# ========= 2. Main function (enhanced logging version) =========\n",
    "\n",
    "def detect_stops_and_trips(\n",
    "    df: pd.DataFrame,\n",
    "    G: int = 1,\n",
    "    min_stop_duration_slots: int = 2,  # (core parameter) duration in time slots\n",
    "    max_grid_gap: int = 1,              # (core parameter) neighboring grid definition\n",
    "    include_stops_in_trips: bool = False,\n",
    "    place_col: str = 'place',\n",
    "    verbose: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    (Main function – enhanced logging version)\n",
    "    Prints elapsed time and shape after each step.\n",
    "    \"\"\"\n",
    "    t_start = time.perf_counter()\n",
    "    if verbose:\n",
    "        tqdm.write(f\"--- detect_stops_and_trips (G={G}, min_duration={min_stop_duration_slots}, max_gap={max_grid_gap}) ---\")\n",
    "    \n",
    "    total_steps = 8\n",
    "    pbar = tqdm(total=total_steps, disable=not verbose, desc=\"Detect stops & trips\")\n",
    "\n",
    "    # Step 1) Copy and type check\n",
    "    t_step = time.perf_counter()\n",
    "    if verbose: tqdm.write(\"[1/8] Copy & ensure dtypes ...\")\n",
    "    df = df.copy()\n",
    "    ensure_int_types(df)\n",
    "    if verbose: tqdm.write(f\"  > Done. ({(time.perf_counter()-t_step):.2f}s) | df.shape={df.shape}\")\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Step 2) Build/confirm place (ensure xg, yg columns exist)\n",
    "    t_step = time.perf_counter()\n",
    "    if verbose: tqdm.write(f\"[2/8] Build/confirm place with G={G} (ensure xg, yg)...\")\n",
    "    build_place_from_grid(df, G=G, inplace_col=place_col)\n",
    "    df['xg'] = df['xg'].astype(np.int64)\n",
    "    df['yg'] = df['yg'].astype(np.int64)\n",
    "    if verbose: tqdm.write(f\"  > Done. ({(time.perf_counter()-t_step):.2f}s) | df.shape={df.shape}\")\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Step 3) Sort (!!! critical: must sort by t !!!)\n",
    "    t_step = time.perf_counter()\n",
    "    if verbose: tqdm.write(\"[3/8] Sort by (uid, d, t) ...\")\n",
    "    df.sort_values(['uid','d','t'], kind='mergesort', inplace=True)\n",
    "    if verbose: tqdm.write(f\"  > Done. ({(time.perf_counter()-t_step):.2f}s)\")\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Step 4) Compute consecutive run_id (spatial-only)\n",
    "    t_step = time.perf_counter()\n",
    "    if verbose:\n",
    "        tqdm.write(f\"[4/8] Compute consecutive run_id (spatial-only, max_gap={max_grid_gap})...\")\n",
    "    df['run_id'] = consecutive_run_id_spatial_only(\n",
    "        df,\n",
    "        by_cols=('uid','d'),\n",
    "        xg_col='xg',\n",
    "        yg_col='yg',\n",
    "        max_grid_gap=max_grid_gap\n",
    "    )\n",
    "    if verbose: tqdm.write(f\"  > Done. ({(time.perf_counter()-t_step):.2f}s)\")\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Step 5) Aggregate run-level statistics\n",
    "    t_step = time.perf_counter()\n",
    "    if verbose: tqdm.write(\"[5/8] Aggregate run stats ...\")\n",
    "    grp_keys = ['uid','d','run_id']\n",
    "    run_stats = (\n",
    "        df.groupby(grp_keys, sort=False)\n",
    "          .agg(\n",
    "              place=(place_col,'first'),\n",
    "              xg=('xg', 'first'),\n",
    "              yg=('yg', 'first'),\n",
    "              start_t=('t','min'),\n",
    "              end_t=('t','max'),\n",
    "              run_len=('t','size')\n",
    "          )\n",
    "          .reset_index()\n",
    "    )\n",
    "    if verbose: tqdm.write(f\"  > Done. ({(time.perf_counter()-t_step):.2f}s) | run_stats.shape={run_stats.shape}\")\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Step 6) Label STOP / MOVE (duration-based)\n",
    "    t_step = time.perf_counter()\n",
    "    if verbose:\n",
    "        tqdm.write(f\"[6/8] Label STOP (duration >= {min_stop_duration_slots}) / MOVE ...\")\n",
    "        \n",
    "    run_stats['duration'] = (run_stats['end_t'] - run_stats['start_t']) + 1 \n",
    "    run_stats['is_stop'] = (run_stats['duration'] >= min_stop_duration_slots)\n",
    "    run_stats['segment_type'] = np.where(run_stats['is_stop'], 'STOP', 'MOVE')\n",
    "\n",
    "    run_stats['stop_seq'] = (\n",
    "        run_stats['is_stop']\n",
    "        .groupby([run_stats['uid'], run_stats['d']], sort=False)\n",
    "        .cumsum()\n",
    "        .where(run_stats['is_stop'], other=np.nan)\n",
    "        .astype(float)\n",
    "    )\n",
    "    if verbose: \n",
    "        n_stops = int(run_stats['is_stop'].sum())\n",
    "        tqdm.write(f\"  > Done. ({(time.perf_counter()-t_step):.2f}s) | Found {n_stops:,} STOPS\")\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Step 7) Compute trip_seq\n",
    "    t_step = time.perf_counter()\n",
    "    if verbose: tqdm.write(\"[7/8] Compute trip_seq for MOVE segments ...\")\n",
    "    run_stats = run_stats.sort_values(['uid','d','start_t'], kind='mergesort')\n",
    "\n",
    "    cum_stops = (\n",
    "        run_stats['is_stop']\n",
    "        .groupby([run_stats['uid'], run_stats['d']], sort=False)\n",
    "        .cumsum()\n",
    "        .astype(np.int32)\n",
    "    )\n",
    "    run_stats['trip_seq'] = np.where(run_stats['segment_type']=='MOVE', cum_stops, np.nan).astype(float)\n",
    "\n",
    "    if include_stops_in_trips:\n",
    "        run_stats['trip_seq'] = np.where(\n",
    "            run_stats['segment_type']=='STOP',\n",
    "            cum_stops,\n",
    "            run_stats['trip_seq']\n",
    "        ).astype(float)\n",
    "    if verbose: tqdm.write(f\"  > Done. ({(time.perf_counter()-t_step):.2f}s)\")\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Step 8) Merge run-level fields back to row-level df\n",
    "    t_step = time.perf_counter()\n",
    "    if verbose: tqdm.write(\"[8/8] Merge run-level fields back to row-level ...\")\n",
    "    merge_cols = ['uid','d','run_id','run_len','duration','is_stop','stop_seq','segment_type','trip_seq']\n",
    "    \n",
    "    for col in merge_cols:\n",
    "        if col not in run_stats.columns:\n",
    "            raise KeyError(f\"Internal error: missing column {col} in run_stats\")\n",
    "\n",
    "    df = df.merge(\n",
    "        run_stats[merge_cols],\n",
    "        on=['uid','d','run_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    df['is_stop'] = df['is_stop'].astype(bool)\n",
    "    df['run_len'] = df['run_len'].astype(np.int32)\n",
    "    df['duration'] = df['duration'].astype(np.int32)\n",
    "    df['stop_seq'] = df['stop_seq'].astype(float)\n",
    "    df['traj_seq'] = df['trip_seq'].astype(float)\n",
    "    df.drop(columns=['trip_seq'], inplace=True)\n",
    "    if verbose: tqdm.write(f\"  > Done. ({(time.perf_counter()-t_step):.2f}s) | final df.shape={df.shape}\")\n",
    "    pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    if verbose:\n",
    "        n_runs = len(run_stats)\n",
    "        n_stops = int(run_stats['is_stop'].sum())\n",
    "        n_moves = int((run_stats['segment_type']=='MOVE').sum())\n",
    "        tqdm.write(f\"\\n[Done] runs={n_runs:,}, STOPs={n_stops:,}, MOVEs={n_moves:,}\")\n",
    "        tqdm.write(f\"--- Total time: {time.perf_counter()-t_start:.2f}s ---\")\n",
    "\n",
    "    return df, run_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be00724",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DURATION = 2\n",
    "GRID_SIZE = 1\n",
    "GRID_GAP = 3\n",
    "\n",
    "detailed_df, run_summary = detect_stops_and_trips(\n",
    "        mob[mob[\"d\"] == 0],\n",
    "        G=GRID_SIZE,\n",
    "        min_stop_duration_slots=MIN_DURATION,\n",
    "        max_grid_gap=GRID_GAP,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "detailed_df.to_parquet(\"data/detailed_df.parquet\", index=False)\n",
    "run_summary.to_parquet(\"data/run_summary.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dbae2f",
   "metadata": {},
   "source": [
    "## Stop-to-Stop Trajectory Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ==========================================================\n",
    "# Configuration\n",
    "# ==========================================================\n",
    "DATA_FILE = \"data/detailed_df.parquet\"\n",
    "OUTPUT_FILE = \"all_stop_move_stop_sequences.parquet\"\n",
    "\n",
    "# ==========================================================\n",
    "# Step 1: Stream loading and basic preprocessing\n",
    "# ==========================================================\n",
    "print(\"Loading basic information from detailed_df...\")\n",
    "try:\n",
    "    metadata_df = pd.read_parquet(DATA_FILE, engine=\"pyarrow\", columns=[\"uid\", \"d\", \"t\"])\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Preparing for streaming read...\")\n",
    "\n",
    "# ==========================================================\n",
    "# Step 2: Define streaming group generator\n",
    "# ==========================================================\n",
    "def stream_person_day_groups(df):\n",
    "    \"\"\"Requires df to be sorted by ['uid', 'd', 't']\"\"\"\n",
    "    last_uid, last_d = None, None\n",
    "    buffer = []\n",
    "    for row in df.itertuples(index=False):\n",
    "        if (row.uid, row.d) != (last_uid, last_d) and buffer:\n",
    "            yield last_uid, last_d, pd.DataFrame(buffer, columns=df.columns)\n",
    "            buffer = []\n",
    "        buffer.append(row)\n",
    "        last_uid, last_d = row.uid, row.d\n",
    "    if buffer:\n",
    "        yield last_uid, last_d, pd.DataFrame(buffer, columns=df.columns)\n",
    "\n",
    "# ==========================================================\n",
    "# Step 3: Load and filter data\n",
    "# ==========================================================\n",
    "print(\"Loading full trajectory table and sorting...\")\n",
    "\n",
    "cols_needed = [\"uid\", \"x\", \"y\", \"d\", \"t\", \"stop_seq\", \"traj_seq\"]\n",
    "df_iter = pd.read_parquet(DATA_FILE, columns=cols_needed)\n",
    "\n",
    "# ✅ Keep only d == 0\n",
    "df_iter = df_iter[df_iter['d'] == 0].copy()\n",
    "\n",
    "# Filter valid points and sort\n",
    "df_iter = df_iter[(df_iter['stop_seq'] > 0) | (df_iter['traj_seq'] > 0)].sort_values(by=['uid', 'd', 't'])\n",
    "print(f\"Base data row count (d=0 only): {len(df_iter):,}\")\n",
    "\n",
    "# ==========================================================\n",
    "# Step 4: Prepare output file\n",
    "# ==========================================================\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    os.remove(OUTPUT_FILE)\n",
    "\n",
    "schema_df = df_iter.head(0).copy()\n",
    "schema_df['trip_group_uid'] = pd.Series(dtype='int32')\n",
    "schema_df['trip_group_d'] = pd.Series(dtype='int32')\n",
    "schema_df['trip_group_id'] = pd.Series(dtype='int64')\n",
    "schema_df['global_seq_id'] = pd.Series(dtype='int64')   # ✅ Global auto-increment ID column\n",
    "# Define schema\n",
    "pa_schema = pa.Table.from_pandas(schema_df, preserve_index=False).schema\n",
    "del schema_df\n",
    "gc.collect()\n",
    "\n",
    "total_trips_found = 0\n",
    "global_seq_id = 0   # ✅ Initialize global counter\n",
    "\n",
    "# ==========================================================\n",
    "# Step 5: Main loop (streaming)\n",
    "# ==========================================================\n",
    "with pq.ParquetWriter(OUTPUT_FILE, pa_schema, compression='snappy') as writer:\n",
    "    for uid, d, daily_table in tqdm(stream_person_day_groups(df_iter), desc=\"Processing (uid, day)\"):\n",
    "        trip_ids = daily_table.loc[daily_table['traj_seq'] > 0, 'traj_seq'].unique()\n",
    "        if len(trip_ids) == 0:\n",
    "            continue\n",
    "\n",
    "        for trip_id_float in trip_ids:\n",
    "            trip_id = int(trip_id_float)\n",
    "            sub_move = daily_table[daily_table['traj_seq'] == trip_id].copy()\n",
    "            prev_stop = daily_table[daily_table['stop_seq'] == trip_id]\n",
    "            next_stop = daily_table[daily_table['stop_seq'] == trip_id + 1]\n",
    "            prev_stop_last = prev_stop.tail(1) if not prev_stop.empty else pd.DataFrame()\n",
    "            next_stop_first = next_stop.head(1) if not next_stop.empty else pd.DataFrame()\n",
    "            sub_df = pd.concat([prev_stop_last, sub_move, next_stop_first], ignore_index=True)\n",
    "            \n",
    "            if not sub_df.empty:\n",
    "                global_seq_id += 1  # ✅ Increment global ID\n",
    "                sub_df = sub_df.assign(\n",
    "                    trip_group_uid=uid,\n",
    "                    trip_group_d=d,\n",
    "                    trip_group_id=trip_id,\n",
    "                    global_seq_id=global_seq_id  # ✅ Assign global ID\n",
    "                )\n",
    "                writer.write_table(pa.Table.from_pandas(sub_df, schema=pa_schema, preserve_index=False))\n",
    "                total_trips_found += 1\n",
    "\n",
    "        del daily_table\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\n--- ✅ Processing completed ---\")\n",
    "print(f\"[Success] Saved {total_trips_found:,} stop-to-stop trajectories to {OUTPUT_FILE}\")\n",
    "\n",
    "# ==========================================================\n",
    "# Step 6: Verification\n",
    "# ==========================================================\n",
    "print(\"\\nVerifying output file...\")\n",
    "df_all = pd.read_parquet(OUTPUT_FILE)\n",
    "print(f\"Verification passed, total rows: {len(df_all):,}\")\n",
    "print(\"Number of unique trip groups:\", len(df_all.groupby(['trip_group_uid', 'trip_group_d', 'trip_group_id'])))\n",
    "print(\"Maximum global_seq_id:\", df_all['global_seq_id'].max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
